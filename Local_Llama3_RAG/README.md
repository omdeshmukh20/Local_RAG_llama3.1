Chat with Webpage using Local Llama3 and RAG

This project allows users to chat with any webpage using local Llama3 and Retrieval-Augmented Generation (RAG). The system extracts webpage content, splits it into chunks, generates embeddings, stores them in a FAISS vector database, retrieves relevant information based on user queries, and generates responses using the Llama3 model.

ğŸš€ Features

Load Webpage Data: Extracts text from a given URL.

Text Chunking: Splits the text into manageable pieces for efficient retrieval.

Embeddings & Vector Storage: Converts text into vectors and stores them in FAISS.

Retrieval-Augmented Generation (RAG): Searches for relevant content before responding.

Local Llama3 Model: Generates human-like responses.

Streamlit UI: Interactive web interface for querying webpage content.

ğŸ“‚ Project Structure

Local_Llama3_RAG/

â”‚â”€â”€ app.py               # Main entry point (Run this file)

â”‚â”€â”€ loader.py            # Loads webpage content

â”‚â”€â”€ splitter.py          # Splits text into chunks

â”‚â”€â”€ embedding.py         # Converts text into vectors

â”‚â”€â”€ model.py             # Llama3 model for response generation

â”‚â”€â”€ retriever.py         # Retrieves relevant info from the database

â”‚â”€â”€ requirements.txt     # Required Python libraries

â”‚â”€â”€ README.md            # Project documentation

ğŸ›  Installation

1ï¸âƒ£ Clone the Repository

git clone https://github.com/yourusername/Local_Llama3_RAG.git
cd Local_Llama3_RAG

2ï¸âƒ£ Install Dependencies

pip install -r requirements.txt

3ï¸âƒ£ Run the Application

streamlit run app.py

ğŸ“ How It Works

User Inputs a Webpage URL â†’ The system extracts webpage text.

Text is Chunked â†’ The extracted content is split into small parts for better searchability.

Embeddings are Generated â†’ The text chunks are converted into vector embeddings using Llama3.

Vectors are Stored in FAISS â†’ The FAISS vector database helps in quick retrieval of relevant text.

User Asks a Question â†’ The system searches for the most relevant content.

Llama3 Generates a Response â†’ The model generates an answer based on the retrieved context.

ğŸ“Œ Example Usage

Enter a webpage URL.

Click "Load Webpage".

Enter a question related to the webpage content.

Get a response powered by local Llama3 + RAG.

ğŸ“œ License

This project is licensed under the MIT License.

âœ¨ Contributions

Contributions are welcome! Feel free to submit issues or pull requests.

ğŸ“ Contact

For any queries, reach out via GitHub or email at -omdeshukh2026@gmail.com.
